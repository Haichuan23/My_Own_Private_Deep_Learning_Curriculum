# Chapter 5: PPO

Implementation:https://colab.research.google.com/drive/1PusO1bxggG8LX3XhvYzMwDi3td603Dlk?usp=sharing

This implementation is purely Monte-Carlo but some popular version of PPO will mix boostrapping and Monte Carlo. 


PPO is a on policy reinforcement learning algorithm that is widely used in tasks like reinforcement learning from human feedback (RLHF).

Several tips on PPO implementation:
(1) One file should contain the PPO class; the other file should define how your PPO agent interacts with the environment and evaluate your policy performance.
(2) There are two cases for PPO implementation: in the discrete case, your policy is a categorical distribution over the action space; in the continuous case, your policy is a Gaussian distribution, and the policy network outputs the mean of your Gaussian.
(3) PPO algorithm collects an episode of rewards into a buffer, so that you can compute the discounted return in an episode. The critic is used to compute the value of the state, and the difference between rewards (current + future discounted) and predicted state value is the advantage. Critic loss is the MSE loss between rewards and predicted state value for the current policy. The policy needs to select actions with larger advantage (defined as difference between true episode level rewards and the old critic's evaluation of states generated by current policy). Clipping is usually implemented to stabilize training. There is often a term incentivizing exploration. 
